# Survey Archetypes: Transform Data Patterns into AI Personas

> **Discover hidden audience segments in your survey data, then simulate how they'll respond to new questionsâ€”without surveying anyone again.**

---

## ğŸ¯ The Problem

You've run a survey and collected responses. Now you want to:

- **Test new questions** without re-surveying everyone
- **Understand audience segments** beyond simple demographics  
- **Predict reactions** to new products, policies, or messaging
- **Generate larger datasets** for statistical validity

Traditional approaches fall short:

âŒ **Manual personas are subjective** - "Sarah, 25, tech-savvy" lacks rigor  
âŒ **Simple clustering misses nuance** - People aren't just one type  
âŒ **Small samples limit analysis** - Hard to detect patterns with 50 respondents

---

## âœ¨ The Solution

**Survey Archetypes** discovers the fundamental "personality types" hidden in your data, converts them into believable AI personas, then deploys them as intelligent agents that can answer *new* questions while maintaining their worldview.

### What You Get

1. **Data-Driven Personas**: Not guessedâ€”mathematically discovered from response patterns
2. **AI Survey Agents**: Personas that can think and answer new questions consistently  
3. **Synthetic Populations**: Generate 1,000+ responses from a 50-person survey
4. **Mock Survey Mode**: Start with hypothetical archetypes to design and validate surveys

### Perfect For

âœ… **Product managers** testing concepts before launch  
âœ… **Researchers** expanding small datasets  
âœ… **Survey designers** validating instruments before fielding  
âœ… **Policy analysts** predicting stakeholder reactions  
âœ… **Market researchers** understanding audience segments

---

## ğŸš€ Quick Start

### Prerequisites & Installation

**1. Install Ollama** (local LLM for persona generation):

```bash
# Visit: https://ollama.ai and download for your OS
# After installation:
ollama pull gemma3:4b
ollama serve  # Keep running in background
```

**2. Install Survey Archetypes:**

```bash
git clone https://github.com/yourusername/survey-archetypes
cd survey-archetypes
pip install -r requirements.txt

# Install frontend
cd frontend
npm install
cd ..
```

### Launch the Application

**Terminal 1 - Backend:**
```bash
python api/server.py
# â†’ Backend running at http://localhost:8000
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
# â†’ Frontend at http://localhost:5173
```

**Open browser:** http://localhost:5173

---

## ğŸ”„ Workflow Overview

The web interface guides you through 7 stages:

| Stage | Purpose | Key Actions | Duration |
|-------|---------|-------------|----------|
| **0. Archetypes** | Define or edit initial personality types | Edit patterns, weights, demographic context | 2-5 min |
| **1. Setup** | Configure questions and parameters | Load templates or create custom questions | 3-5 min |
| **2. Discovery** | Find optimal number of archetypes (k) | Run k-analysis (k=2 to k=8), select best k | 1-2 min |
| **3. Personas** | Generate rich AI personas from patterns | Review/edit LLM-generated descriptions | 2-3 min |
| **4. Survey** | Define new questions for validation | Add Likert/categorical/ordinal questions | 2-3 min |
| **5. Calibration** | Agents answer questions multiple times | Watch live as probability distributions build | 1-2 min |
| **6. Simulation** | Generate full synthetic population | Configure size (default: 1,000), run simulation | 30 sec |
| **7. Analysis** | Visualize & export results | Download CSV, generate plots | 1 min |

**Total Time:** ~15-20 minutes from start to synthetic dataset

---

## ğŸ”¬ How It Works

### Stage 1: Pattern Discovery (Archetypal Analysis)

**What happens:** Mathematical decomposition finds "pure personality types" in your data.

**The Math:**
```
Survey Data = Mixing Weights Ã— Pure Archetypes

Where:
- Survey Data: How each person answered (n_people Ã— n_questions)
- Mixing Weights: How much of each type is in each person
- Pure Archetypes: The extreme personalities everyone blends from
```

**Why archetypes, not clustering?**

| Method | What It Finds | Example |
|--------|---------------|---------|
| **K-Means Clustering** | Groups of similar people with hard boundaries | "You're in Group A or Group B" |
| **Archetypal Analysis** | Extreme types that everyone is a mixture of | "You're 70% Type A + 30% Type B" |

**Output Example:**
```
Your 200 respondents decompose into:
- Type A: Trust institutions, risk-averse (40% of sample)
- Type B: Tech optimist, environmentalist (30%)  
- Type C: Traditional, skeptical of change (20%)
- Type D: Disengaged, neutral on most topics (10%)

Person #137 = 70% Type A + 30% Type B
```

**Key Insight:** Like primary colors mixing to create all shadesâ€”any respondent is a weighted combination of these pure archetypes.

---

### Stage 2: Semantic Translation (LLM Persona Generation)

**What happens:** Local LLM converts mathematical patterns into human narratives.

**Transformation Example:**

```
Mathematical Pattern (Type B):
â”œâ”€ Q1 (Trust govt): 2/5
â”œâ”€ Q2 (Tech optimism): 5/5
â”œâ”€ Q3 (Tradition): 2/5
â”œâ”€ Q4 (Ecology): 5/5
â””â”€ Q5 (Risk-taking): 4/5

        â†“ LLM Translation â†“

Generated Persona:
"Leo Maxwell, 22
Computer Science Junior at NYU
Values: Innovation, autonomy, environmental sustainability
Fears: Institutional overreach, stagnation, ecological collapse
Worldview: Deeply skeptical of government but optimistic about 
technology solving problems. Believes in taking calculated risks 
for progress while protecting the planet."
```

**Features:**
- âœ… **Contrastive Generation**: Each persona explicitly differs from previous ones
- âœ… **Demographic Constraints**: Enforces realistic age, occupation, location
- âœ… **Explainable**: Shows why this persona gave specific scores
- âœ… **Editable**: Refine manuallyâ€”system regenerates reasoning automatically

---

### Stage 3: Agent Construction (Survey-Taking AI)

**What happens:** Personas become AI agents that can answer new questions using Chain-of-Thought reasoning.

**Agent Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PERSONA (System Prompt)            â”‚
â”‚  "You are Leo Maxwell, 22...        â”‚
â”‚   Values: Innovation | Fears: ..."  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NEW QUESTION                        â”‚
â”‚  "Trust social media companies?"    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CHAIN-OF-THOUGHT REASONING          â”‚
â”‚  "I'm optimistic about tech (Q2=5)  â”‚
â”‚   BUT distrust institutions (Q1=2)  â”‚
â”‚   â†’ Social media = tech + corporate â”‚
â”‚   â†’ Leaning toward skepticism..."   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANSWER: 2/5 (Disagree)             â”‚
â”‚  + Explainable reasoning            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why AI agents vs simple sampling?**

Traditional sampling: `if archetype == "Progressive": answer = random.choice([4,5])`  
âŒ Can't handle new questions | âŒ No reasoning | âŒ No consistency

AI Agent approach: Reasons about each question in context  
âœ… Answers unseen questions | âœ… Maintains worldview | âœ… Explains logic | âœ… Adapts to phrasing

**Key Design:**
- **Multiple Sampling**: Each agent answers 10+ times per question
- **Calibration**: Builds probability distributions (not single answers)
- **Local LLM**: Privacy-first (Ollama), no data leaves your machine. Can be used as much as you want

---

### Stage 4: Calibration & Simulation

**Calibration:** Agents answer each new question multiple times to capture uncertainty.

```
Leo (Type B) answers "Trust social media?" 10 times:
[2, 2, 3, 2, 2, 1, 2, 3, 2, 2]

Statistics:
- Modal answer: 2 (most common)
- P(answer 1): 10%
- P(answer 2): 70%  â† Most likely
- P(answer 3): 20%
- Mean: 2.1, StdDev: 0.5
```

**Simulation:** Generate full population using calibrated distributions.

```
1. Assign archetypes by weight:
   Respondent #1 â†’ Type A (40% weight)
   Respondent #2 â†’ Type B (30% weight)
   ...

2. Sample responses from calibrated distributions:
   Respondent #2 (Type B) on Q: "Trust social media?"
   â†’ Samples from [10% chance=1, 70% chance=2, 20% chance=3]
   â†’ Gets "2"

3. Result: 1,000 respondents with realistic variance
```

---

### Stage 5: Validation & Export

**Validation Methods:**
- Distribution comparison (simulated vs expected)
- RÂ² scores (how well archetypes explain variance)
- Statistical tests (are type differences significant?)
- Visual plots (side-by-side real vs synthetic)

**Example:**
```
Original Survey (200 people):
  Progressive: 40% | Traditionalist: 20%

Simulated (1,000 people):  
  Progressive: 39.8% | Traditionalist: 20.3%
  
âœ… Distributions match within statistical error
âœ… RÂ² = 0.87 (strong pattern preservation)
```

**Export:**
- `simulated_survey.csv` - Full synthetic dataset (1,000+ rows)
- `calibration_data.csv` - Probability distributions per agent
- `personas.json` - Complete persona descriptions
- Visualization plots (distribution comparisons)

---

## ğŸ“ Project Structure

```
survey_archetypes/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ config/              # JSON configurations
â”‚   â”‚   â”œâ”€â”€ questions/       # Question templates
â”‚   â”‚   â”œâ”€â”€ archetypes/      # Archetype definitions
â”‚   â”‚   â””â”€â”€ system_config.json
â”‚   â”œâ”€â”€ input/               # Your CSV files
â”‚   â””â”€â”€ output/              # Results & plots
â”‚
â”œâ”€â”€ config/                  # Python config layer
â”‚   â”œâ”€â”€ settings.py          # Paths & parameters
â”‚   â”œâ”€â”€ questions.py         # Question schemas
â”‚   â””â”€â”€ loader.py            # JSON loader
â”‚
â”œâ”€â”€ core/                    # Core algorithms
â”‚   â”œâ”€â”€ archetypal_analyzer.py  # NMF/PCHA decomposition
â”‚   â””â”€â”€ encoding.py          # Data type conversion
â”‚
â”œâ”€â”€ generators/              # Creation modules
â”‚   â”œâ”€â”€ survey_data_generator.py
â”‚   â””â”€â”€ persona_generator.py
â”‚
â”œâ”€â”€ agents/                  # AI behavior
â”‚   â””â”€â”€ survey_agent.py      # Survey-taking agents
â”‚
â”œâ”€â”€ simulation/              # Population simulation
â”‚   â””â”€â”€ population_simulator.py
â”‚
â”œâ”€â”€ analysis/                # Visualization
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ api/                     # Web backend (FastAPI)
â”‚   â””â”€â”€ server.py
â”‚
â”œâ”€â”€ frontend/                # React UI
â”‚   â””â”€â”€ src/App.jsx
â”‚
â””â”€â”€ requirements.txt
```

---

## ğŸ“‹ Usage Guidelines

### What This Tool CAN Do

âœ… Discover mathematically-grounded audience segments  
âœ… Simulate responses to new questions within same domain  
âœ… Generate statistically valid synthetic datasets  
âœ… Test survey designs before fielding  
âœ… Scale small datasets (50 â†’ 1,000+)

### What This Tool CANNOT Do

âŒ Replace real human insight and qualitative research  
âŒ Predict responses to completely unrelated questions  
âŒ Capture complex emotions or unstructured feedback  
âŒ Guarantee AI reasoning perfectly matches human reasoning  
âŒ Work with fewer than ~30-50 initial respondents

### Best Practices

**Data Quality:**
- Minimum 50 respondents recommended
- Include diverse question types (Likert + categorical)
- Ensure questions are clear and well-designed

**Validation:**
- Always validate against held-out real data when possible
- Check archetype distributions make intuitive sense
- Review generated personas for appropriateness

**Usage:**
- Use for hypothesis generation, not final decisions
- Test new questions *related* to original survey domain
- Combine with qualitative research methods
- Don't extrapolate too far from training data

**LLM Settings:**
- Keep Ollama running during all operations
- Larger models (7B+) produce more consistent reasoning
- Default temperature (0.7) balances creativity/consistency
- Chain-of-Thought prompting is crucial for quality

---

## ğŸ”§ Troubleshooting

### Ollama Connection Issues

**Error:** `Connection refused to localhost:11434`

```bash
# Check if running:
ollama list

# Start server:
ollama serve

# Verify model:
ollama pull gemma3:4b
```

### Persona Generation Slow/Fails

**Solutions:**
1. Use smaller model: `ollama pull gemma3:2b`
2. Increase timeout in `data/config/system_config.json`:
   ```json
   "ollama": {"timeout": 300}
   ```
3. Check Ollama logs for errors

### Frontend Can't Connect to Backend

```bash
# 1. Verify backend running:
python api/server.py  # Should show port 8000

# 2. Check API_URL in frontend/src/App.jsx:
const API_URL = 'http://localhost:8000/api';

# 3. Clear browser cache
```

### Synthetic Data Seems Random

**Fixes:**
1. Increase `calibration_samples` to 15-20
2. Review initial archetypes (Tab 0)
3. Lower `temperature_agent` in config
4. Ensure training data has variance

### Python Import Errors

```bash
pip install -r requirements.txt
pip install ollama
```

---

## ğŸ“ Learn More

### Example Applications

**Market Research:**
- Test product concepts on synthetic consumer panels
- Predict adoption rates by segment before launch

**Public Policy:**
- Simulate stakeholder reactions to regulations
- Identify opposition sources and design communication strategies

**Academic Research:**
- Generate training data for ML models
- Conduct power analysis with synthetic samples
- Validate survey instruments before fielding

### Related Research

- **Cutler & Breiman (1994)**: "Archetypal Analysis" - Original method
- **Wei et al. (2022)**: "Chain-of-Thought Prompting" - CoT reasoning foundation
- **El Emam et al. (2020)**: "Practical Synthetic Data Generation" - Validation best practices

---

## ğŸ¤ Contributing

We welcome contributions! Priority areas:

1. **Better archetypal methods**: Implement true PCHA (not NMF approximation)
2. **Advanced persona generation**: RAG or fine-tuned models
3. **Validation metrics**: More sophisticated similarity measures
4. **Real-world datasets**: Test on diverse survey types
5. **Documentation**: More examples and tutorials

---

## ğŸ“„ License

MIT License - Free for research and commercial use.

---

## ğŸ“§ Support

**Issues?** 
1. Check Troubleshooting section above
2. Verify Ollama is running: `ollama list`
3. Check configuration: `data/config/system_config.json`
4. Run module tests: `python core/archetypal_analyzer.py`

---

**Ready to discover your hidden audience segments?** ğŸš€

```bash
python api/server.py  # Start backend
cd frontend && npm run dev  # Start frontend
# Open http://localhost:5173
```